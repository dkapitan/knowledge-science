
@misc{datacentric,
  title = {Data-Centric {{AI Resource Hub}}},
  journal = {Data-centric AI Resource Hub},
  abstract = {Find the latest developments and best practices compiled here, so you can begin your Data-centric AI journey!},
  howpublished = {https://datacentricai.org/},
  langid = {english},
  file = {/Users/dkapitan/Zotero/storage/K8K987BV/datacentricai.org.html}
}

@misc{fletcher2020knowledge,
  title = {Knowledge {{Scientists}}: {{Unlocking}} the Data-Driven Organization},
  shorttitle = {Knowledge {{Scientists}}},
  author = {Fletcher, George and Groth, Paul and Sequeda, Juan},
  year = {2020},
  month = apr,
  number = {arXiv:2004.07917},
  eprint = {2004.07917},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2004.07917},
  abstract = {Organizations across all sectors are increasingly undergoing deep transformation and restructuring towards data-driven operations. The central role of data highlights the need for reliable and clean data. Unreliable, erroneous, and incomplete data lead to critical bottlenecks in processing pipelines and, ultimately, service failures, which are disastrous for the competitive performance of the organization. Given its central importance, those organizations which recognize and react to the need for reliable data will have the advantage in the coming decade. We argue that the technologies for reliable data are driven by distinct concerns and expertise which complement those of the data scientist and the data engineer. Those organizations which identify the central importance of meaningful, explainable, reproducible, and maintainable data will be at the forefront of the democratization of reliable data. We call the new role which must be developed to fill this critical need the Knowledge Scientist. The organizational structures, tools, methodologies and techniques to support and make possible the work of knowledge scientists are still in their infancy. As organizations not only use data but increasingly rely on data, it is time to empower the people who are central to this transformation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Databases,Computer Science - General Literature},
  file = {/Users/dkapitan/Zotero/storage/EUF2XVXF/Fletcher et al. - 2020 - Knowledge Scientists Unlocking the data-driven or.pdf;/Users/dkapitan/Zotero/storage/HWE64L59/2004.html}
}

@misc{gebru2021datasheets,
  title = {Datasheets for {{Datasets}}},
  author = {Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan, Jennifer Wortman and Wallach, Hanna and Daum{\'e} III, Hal and Crawford, Kate},
  year = {2021},
  month = dec,
  number = {arXiv:1803.09010},
  eprint = {1803.09010},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1803.09010},
  abstract = {The machine learning community currently has no standardized process for documenting datasets, which can lead to severe consequences in high-stakes domains. To address this gap, we propose datasheets for datasets. In the electronics industry, every component, no matter how simple or complex, is accompanied with a datasheet that describes its operating characteristics, test results, recommended uses, and other information. By analogy, we propose that every dataset be accompanied with a datasheet that documents its motivation, composition, collection process, recommended uses, and so on. Datasheets for datasets will facilitate better communication between dataset creators and dataset consumers, and encourage the machine learning community to prioritize transparency and accountability.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Databases,Computer Science - Machine Learning},
  file = {/Users/dkapitan/Zotero/storage/NIHXF4RQ/Gebru et al. - 2021 - Datasheets for Datasets.pdf;/Users/dkapitan/Zotero/storage/EXJBU7GS/1803.html}
}

@misc{ruiz201780,
  title = {The 80/20 Data Science Dilemma},
  author = {Ruiz, Armand},
  year = {2017},
  month = sep,
  journal = {InfoWorld},
  abstract = {Most data scientists spend only 20 percent of their time on actual data analysis and 80 percent of their time finding, cleaning, and reorganizing huge amounts of data, which is an inefficient data strategy},
  howpublished = {https://www.infoworld.com/article/3228245/the-80-20-data-science-dilemma.html},
  langid = {english}
}

@book{schreiber1999knowledge,
  title = {Knowledge {{Engineering}} and {{Management}}: {{The CommonKADS Methodology}}},
  shorttitle = {Knowledge {{Engineering}} and {{Management}}},
  author = {Schreiber, Guus and Akkermans, Hans and Anjewierden, Anjo and {de Hoog}, Robert and Shadbolt, Nigel R. and {Van de Velde}, Walter and Wielinga, B. J.},
  year = {1999},
  publisher = {{The MIT Press}},
  doi = {10.7551/mitpress/4073.001.0001},
  isbn = {978-0-262-28323-6},
  langid = {english},
  file = {/Users/dkapitan/Zotero/storage/BVVU8WYM/Schreiber et al. - 1999 - Knowledge Engineering and Management The CommonKA.pdf}
}

@article{sculleyhidden,
  title = {Hidden {{Technical Debt}} in {{Machine Learning Systems}}},
  author = {Sculley, D and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael and Crespo, Jean-Fran{\c c}ois and Dennison, Dan},
  pages = {9},
  abstract = {Machine learning offers a fantastically powerful toolkit for building useful complex prediction systems quickly. This paper argues it is dangerous to think of these quick wins as coming for free. Using the software engineering framework of technical debt, we find it is common to incur massive ongoing maintenance costs in real-world ML systems. We explore several ML-specific risk factors to account for in system design. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, configuration issues, changes in the external world, and a variety of system-level anti-patterns.},
  langid = {english}
}

@article{stonebrakerdata,
  title = {Data {{Integration}}: {{The Current Status}} and the {{Way Forward}}},
  author = {Stonebraker, Michael and Ilyas, Ihab F},
  pages = {7},
  abstract = {We discuss scalable data integration challenges in the enterprise inspired by our experience at Tamr1. We use multiple real customer examples to highlight the technical difficulties around building a deployable and usable data integration software that tackles the data silos problem. We also highlight the practical aspects involved in using machine learning to enable automating manual or rule-based processes for data integration tasks, such as schema mapping, classification, and deduplication.},
  langid = {english},
  file = {/Users/dkapitan/Zotero/storage/SH54HFDW/Stonebraker and Ilyas - Data Integration The Current Status and the Way F.pdf}
}

@article{studer1998knowledge,
  title = {Knowledge Engineering: {{Principles}} and Methods},
  shorttitle = {Knowledge Engineering},
  author = {Studer, Rudi and Benjamins, V. Richard and Fensel, Dieter},
  year = {1998},
  month = mar,
  journal = {Data \& Knowledge Engineering},
  volume = {25},
  number = {1},
  pages = {161--197},
  issn = {0169-023X},
  doi = {10.1016/S0169-023X(97)00056-6},
  abstract = {This paper gives an overview of the development of the field of Knowledge Engineering over the last 15 years. We discuss the paradigm shift from a transfer view to a modeling view and describe two approaches which considerably shaped research in Knowledge Engineering: Role-limiting Methods and Generic Tasks. To illustrate various concepts and methods which evolved in recent years we describe three modeling frameworks: CommonKADS, MIKE and PROT\'EG\'E-II. This description is supplemented by discussing some important methodological developments in more detail: specification languages for knowledge-based systems, problem-solving methods and ontologies. We conclude by outlining the relationship of Knowledge Engineering to Software Engineering, Information Integration and Knowledge Management.},
  langid = {english},
  keywords = {Information integration,Knowledge acquisition,Knowledge Engineering,Ontology,Problem-solving method},
  file = {/Users/dkapitan/Zotero/storage/YGYRZ2E4/Studer et al. - 1998 - Knowledge engineering Principles and methods.pdf;/Users/dkapitan/Zotero/storage/EKT2TW9E/S0169023X97000566.html}
}

@article{teperek2018data,
  title = {Data {{Stewardship}} \textendash{} Addressing Disciplinary Data Management Needs},
  author = {Teperek, Marta and Cruz, Maria J. and Verbakel, Ellen and B{\"o}hmer, Jasmin K. and Dunning, Alastair},
  year = {2018},
  month = jan,
  publisher = {{OSF}},
  doi = {10.17605/OSF.IO/MJK9T},
  abstract = {One of the biggest challenges for multidisciplinary research institutions which provide data management support to researchers is addressing disciplinary differences1. Centralised services need to be general enough to cater for all the different flavours of research conducted in an institution. At the same time, focusing on the common denominator means that subject-specific differences and needs may not be effectively addressed. In 2017, Delft University of Technology (TU Delft) embarked on an ambitious Data Stewardship project, aiming to comprehensively address data management needs across a multi-disciplinary campus.  In this practice paper, we describe the principles behind the Data Stewardship project at TU Delft, the progress so far, we identify the key challenges and explain our plans for the future.      Hosted on the Open Science Framework},
  langid = {english},
  file = {/Users/dkapitan/Zotero/storage/73IFTIR3/mjk9t.html}
}


