
@article{augusto2022be,
  title = {To {{Be}} or {{Not}} to {{Be Informed}}, {{That}} Is the {{Question}} of {{O}}/{{Ontology}}},
  author = {Augusto, Luis M.},
  year = {2022},
  journal = {Journal of Knowledge Structures and Systems},
  volume = {3},
  number = {3},
  pages = {3--49}
}

@article{corcho2003methodologies,
  title = {Methodologies, Tools and Languages for Building Ontologies. {{Where}} Is Their Meeting Point?},
  author = {Corcho, Oscar and {Fern{\'a}ndez-L{\'o}pez}, Mariano and {G{\'o}mez-P{\'e}rez}, Asunci{\'o}n},
  year = {2003},
  month = jul,
  journal = {Data \& Knowledge Engineering},
  volume = {46},
  number = {1},
  pages = {41--64},
  issn = {0169-023X},
  doi = {10.1016/S0169-023X(02)00195-7},
  abstract = {In this paper we review and compare the main methodologies, tools and languages for building ontologies that have been reported in the literature, as well as the main relationships among them. Ontology technology is nowadays mature enough: many methodologies, tools and languages are already available. The future work in this field should be driven towards the creation of a common integrated workbench for ontology developers to facilitate ontology development, exchange, evaluation, evolution and management, to provide methodological support for these tasks, and translations to and from different ontology languages. This workbench should not be created from scratch, but instead integrating the technology components that are currently available.},
  langid = {english},
  keywords = {Ontology,Ontology language,Ontology methodology,Ontology tool},
  file = {/Users/dkapitan/Zotero/storage/QRUWGKZ9/Corcho et al. - 2003 - Methodologies, tools and languages for building on.pdf}
}

@book{data,
  title = {Data {{Architecture}}: {{A Primer}} for the {{Data Scientist}}, 2nd {{Edition}}},
  shorttitle = {Data {{Architecture}}},
  abstract = {Over the past 5 years, the concept of big data has matured, data science has grown exponentially, and data architecture has become a standard part of organizational decision-making. Throughout all...},
  isbn = {978-0-12-816917-9},
  langid = {english},
  file = {/Users/dkapitan/Zotero/storage/BHITPR9T/9780128169179.html}
}

@misc{datacentric,
  title = {Data-Centric {{AI Resource Hub}}},
  journal = {Data-centric AI Resource Hub},
  abstract = {Find the latest developments and best practices compiled here, so you can begin your Data-centric AI journey!},
  howpublished = {https://datacentricai.org/},
  langid = {english},
  file = {/Users/dkapitan/Zotero/storage/K8K987BV/datacentricai.org.html}
}

@article{decesare2016boro,
  title = {{{BORO}} as a {{Foundation}} to {{Enterprise Ontology}}},
  author = {{de Cesare}, Sergio and Partridge, Chris},
  year = {2016},
  month = feb,
  journal = {Journal of Information Systems},
  volume = {30},
  number = {2},
  pages = {83--112},
  issn = {0888-7985},
  doi = {10.2308/isys-51428},
  abstract = {Modern business organizations experience increasing challenges in the development and evolution of their enterprise systems. Typical problems include legacy re-engineering, systems integration/interoperability, and the architecting of the enterprise. At the heart of all these problems is enterprise modeling. Many enterprise modeling approaches have been proposed in the literature with some based on ontology. Few however adopt a foundational ontology to underpin a range of enterprise models in a consistent and coherent manner. Fewer still take data-driven re-engineering as their natural starting point for modeling. This is the approach taken by Business Object Reference Ontology (BORO). It has two closely intertwined components: a foundational ontology and a re-engineering methodology. These were originally developed for the re-engineering of enterprise systems and subsequently evolved into approaches to enterprise architecture and systems integration. Together these components are used to systematically unearth reusable and generalized business patterns from existing data. Most of these patterns have been developed for the enterprise context and have been successfully applied in several commercial projects within the financial, defense, and oil and gas industries. BORO's foundational ontology is grounded in philosophy and its metaontological choices (including perdurantism, extensionalism, and possible worlds) follow well-established theories. BORO's re-engineering methodology is rooted in the philosophical notion of grounding; it emerged from the practice of deploying its foundational ontology and has been refined over the last 25 years. This paper presents BORO and its application to enterprise modeling.},
  file = {/Users/dkapitan/Zotero/storage/GDBFKCTC/BORO-as-a-Foundation-to-Enterprise-Ontology.html}
}

@article{demast2022analytical,
  title = {Analytical {{Problem Solving Based}} on {{Causal}}, {{Correlational}} and {{Deductive Models}}},
  author = {{de Mast}, Jeroen and Steiner, Stefan H. and Nuijten, Wim P. M. and Kapitan, Daniel},
  year = {2022},
  month = jan,
  journal = {The American Statistician},
  pages = {1--11},
  publisher = {{Taylor \& Francis}},
  issn = {0003-1305},
  doi = {10.1080/00031305.2021.2023633}
}

@inproceedings{dou2015semantic,
  title = {Semantic Data Mining: {{A}} Survey of Ontology-Based Approaches},
  shorttitle = {Semantic Data Mining},
  booktitle = {Proceedings of the 2015 {{IEEE}} 9th {{International Conference}} on {{Semantic Computing}} ({{IEEE ICSC}} 2015)},
  author = {Dou, Dejing and Wang, Hao and Liu, Haishan},
  year = {2015},
  month = feb,
  pages = {244--251},
  doi = {10.1109/ICOSC.2015.7050814},
  abstract = {Semantic Data Mining refers to the data mining tasks that systematically incorporate domain knowledge, especially formal semantics, into the process. In the past, many research efforts have attested the benefits of incorporating domain knowledge in data mining. At the same time, the proliferation of knowledge engineering has enriched the family of domain knowledge, especially formal semantics and Semantic Web ontologies. Ontology is an explicit specification of conceptualization and a formal way to define the semantics of knowledge and data. The formal structure of ontology makes it a nature way to encode domain knowledge for the data mining use. In this survey paper, we introduce general concepts of semantic data mining. We investigate why ontology has the potential to help semantic data mining and how formal semantics in ontologies can be incorporated into the data mining process. We provide detail discussions for the advances and state of art of ontology-based approaches and an introduction of approaches that are based on other form of knowledge representations.},
  keywords = {Data mining,Ontologies},
  file = {/Users/dkapitan/Zotero/storage/7F3KWLF2/7050814.html}
}

@inproceedings{elhassouni2022ontology,
  title = {Ontology {{Engineering Methodologies}}: {{State}} of the {{Art}}},
  shorttitle = {Ontology {{Engineering Methodologies}}},
  booktitle = {Proceedings of the 5th {{International Conference}} on {{Big Data}} and {{Internet}} of {{Things}}},
  author = {ElHassouni, Jalil and Qadi, Abderrahim El},
  editor = {Lazaar, Mohamed and Duvallet, Claude and Touhafi, Abdellah and Al Achhab, Mohammed},
  year = {2022},
  series = {Lecture {{Notes}} in {{Networks}} and {{Systems}}},
  pages = {59--72},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-031-07969-6_5},
  abstract = {Recently, the use of ontologies has become more popular in both academia and industry fields. However, many ontology projects have failed due to, at least in part, a lack of discipline in the development process; that is, the poorly specified, underspecified, or lack of requirements and evaluation criteria. Therefore, it is reasonable to ask: what is the most prominent methodology to develop an ontology? To answer this question in this paper, we reviewed the most prominent ontology engineering methodologies, and analyzed the most mature and suitable approaches for the development of ontology based on a set of criteria. We also provide a better understanding of ontology engineering methodology, most used ontology engineering methodologies, current prominent methods, and future research scope for standard ontology engineering methodology. This study shows that no methodology enjoys consensus among the community, and none of these methodologies are mature enough and without limitations. Finally, we concluded that Krisnadhi \& Hitzler methodology remains very practical and much more detailed in a step-by-step manner. The fact that it is based on ontology design patterns ensures that there is a trade-off between interoperability, on the one hand, and over-commitment and conflicting requirements on the other hand.},
  isbn = {978-3-031-07969-6},
  langid = {english},
  keywords = {Developing ontology,Knowledge engineering,Ontology engineering methodology}
}

@article{feilmayr2016analysis,
  title = {An Analysis of Ontologies and Their Success Factors for Application to Business},
  author = {Feilmayr, Christina and W{\"o}{\ss}, Wolfram},
  year = {2016},
  month = jan,
  journal = {Data \& Knowledge Engineering},
  volume = {101},
  pages = {1--23},
  issn = {0169-023X},
  doi = {10.1016/j.datak.2015.11.003},
  abstract = {Ontologies have been less successful than they could be in large-scale business applications due to a wide variety of interpretations. This leads to confusion, and consequently, people from various research communities use the term with different \textendash{} sometimes incompatible \textendash{} meanings. This research work analyzes and clarifies the term ontology and points out its difference from taxonomy. By way of two business case studies, both their potential in ontological engineering and the perceived requirements for ontologies are highlighted, and their misuse in research and business is discussed. In order to examine the case for applying ontologies in a specific domain or use case, the main benefits of using ontologies are defined and categorized as technical-centered or user-centered. Key factors that influence the use of ontologies in business applications are derived and discussed. Finally, the paper offers a recommendation for efficiently applying ontologies, including adequate representation languages and an ontological engineering process supported by reference ontologies. To answer the questions of when ontologies should be used, how they can be used efficiently, and when they should not be used, we propose guidelines for selecting an appropriate model, methodology, and tool set to meet customer requirements while making most efficient use of resources.},
  langid = {english},
  keywords = {Data modeling,Data models,Ontologies,Ontology engineering}
}

@misc{fletcher2020knowledge,
  title = {Knowledge {{Scientists}}: {{Unlocking}} the Data-Driven Organization},
  shorttitle = {Knowledge {{Scientists}}},
  author = {Fletcher, George and Groth, Paul and Sequeda, Juan},
  year = {2020},
  month = apr,
  number = {arXiv:2004.07917},
  eprint = {2004.07917},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2004.07917},
  abstract = {Organizations across all sectors are increasingly undergoing deep transformation and restructuring towards data-driven operations. The central role of data highlights the need for reliable and clean data. Unreliable, erroneous, and incomplete data lead to critical bottlenecks in processing pipelines and, ultimately, service failures, which are disastrous for the competitive performance of the organization. Given its central importance, those organizations which recognize and react to the need for reliable data will have the advantage in the coming decade. We argue that the technologies for reliable data are driven by distinct concerns and expertise which complement those of the data scientist and the data engineer. Those organizations which identify the central importance of meaningful, explainable, reproducible, and maintainable data will be at the forefront of the democratization of reliable data. We call the new role which must be developed to fill this critical need the Knowledge Scientist. The organizational structures, tools, methodologies and techniques to support and make possible the work of knowledge scientists are still in their infancy. As organizations not only use data but increasingly rely on data, it is time to empower the people who are central to this transformation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Databases,Computer Science - General Literature},
  file = {/Users/dkapitan/Zotero/storage/EUF2XVXF/Fletcher et al. - 2020 - Knowledge Scientists Unlocking the data-driven or.pdf;/Users/dkapitan/Zotero/storage/HWE64L59/2004.html}
}

@misc{gebru2021datasheets,
  title = {Datasheets for {{Datasets}}},
  author = {Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan, Jennifer Wortman and Wallach, Hanna and Daum{\'e} III, Hal and Crawford, Kate},
  year = {2021},
  month = dec,
  number = {arXiv:1803.09010},
  eprint = {1803.09010},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1803.09010},
  abstract = {The machine learning community currently has no standardized process for documenting datasets, which can lead to severe consequences in high-stakes domains. To address this gap, we propose datasheets for datasets. In the electronics industry, every component, no matter how simple or complex, is accompanied with a datasheet that describes its operating characteristics, test results, recommended uses, and other information. By analogy, we propose that every dataset be accompanied with a datasheet that documents its motivation, composition, collection process, recommended uses, and so on. Datasheets for datasets will facilitate better communication between dataset creators and dataset consumers, and encourage the machine learning community to prioritize transparency and accountability.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Databases,Computer Science - Machine Learning},
  file = {/Users/dkapitan/Zotero/storage/NIHXF4RQ/Gebru et al. - 2021 - Datasheets for Datasets.pdf;/Users/dkapitan/Zotero/storage/EXJBU7GS/1803.html}
}

@misc{hogan2021knowledge,
  title = {Knowledge {{Graphs}}},
  author = {Hogan, Aidan and Blomqvist, Eva and Cochez, Michael and {d'Amato}, Claudia and {de Melo}, Gerard and Gutierrez, Claudio and Gayo, Jos{\'e} Emilio Labra and Kirrane, Sabrina and Neumaier, Sebastian and Polleres, Axel and Navigli, Roberto and Ngomo, Axel-Cyrille Ngonga and Rashid, Sabbir M. and Rula, Anisa and Schmelzeisen, Lukas and Sequeda, Juan and Staab, Steffen and Zimmermann, Antoine},
  year = {2021},
  month = sep,
  eprint = {2003.02320},
  eprinttype = {arxiv},
  primaryclass = {cs},
  doi = {10.1145/3447772},
  abstract = {In this paper we provide a comprehensive introduction to knowledge graphs, which have recently garnered significant attention from both industry and academia in scenarios that require exploiting diverse, dynamic, large-scale collections of data. After some opening remarks, we motivate and contrast various graph-based data models and query languages that are used for knowledge graphs. We discuss the roles of schema, identity, and context in knowledge graphs. We explain how knowledge can be represented and extracted using a combination of deductive and inductive techniques. We summarise methods for the creation, enrichment, quality assessment, refinement, and publication of knowledge graphs. We provide an overview of prominent open knowledge graphs and enterprise knowledge graphs, their applications, and how they use the aforementioned techniques. We conclude with high-level future research directions for knowledge graphs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Databases,Computer Science - Machine Learning},
  file = {/Users/dkapitan/Zotero/storage/ZAELL9TD/Hogan et al. - 2021 - Knowledge Graphs.pdf;/Users/dkapitan/Zotero/storage/L2JS2GXR/2003.html}
}

@inproceedings{idrissi2022formalization,
  title = {On {{The Formalization}} of {{The TOGAF Content MetaModel Using Ontologies}}},
  booktitle = {2022 {{International Conference}} on {{Intelligent Systems}} and {{Computer Vision}} ({{ISCV}})},
  author = {Idrissi, Bouchra El and Tetou, Chadia and Doumi, Karim},
  year = {2022},
  month = may,
  pages = {1--6},
  issn = {2768-0754},
  doi = {10.1109/ISCV54655.2022.9806063},
  abstract = {Ontology, when well designed, provides a semantic model that facilitates human interpretation of models and reduces their ambiguity and complexity. A formal-based standards ontology can be processed automatically by computer programs, providing several benefits such as automatic knowledge discovery and inference, inconsistency detection, and interoperability. Several frameworks have been developed to control the complexity of the enterprise architecture. This paper focuses on the TOGAF framework and precisely on its content metamodel. It analyses and discusses the reported approaches to its formalization using ontologies. Additionally, it proposes some possible future directions to improve this work and enhance its adoption by the industrial world and academic communities.},
  keywords = {Complexity theory,Computational modeling,Enterprise architecture,Knowledge discovery,Ontologies,ontology,Prototypes,semantic web,Semantics,Stakeholders,togaf framework},
  file = {/Users/dkapitan/Zotero/storage/L5AWS6QH/9806063.html}
}

@book{knowledge,
  title = {Knowledge {{Graphs}}},
  abstract = {Applying knowledge in the right context is the most powerful lever businesses can use to become agile, creative, and resilient. Knowledge graphs add context, meaning, and utility to business data....},
  isbn = {978-1-09-810486-3},
  langid = {english},
  file = {/Users/dkapitan/Zotero/storage/YWZMJRNQ/9781098104863.html}
}

@misc{knowledgea,
  title = {Knowledge Graphs},
  journal = {The Alan Turing Institute},
  howpublished = {https://www.turing.ac.uk/research/interest-groups/knowledge-graphs},
  langid = {english},
  file = {/Users/dkapitan/Zotero/storage/V4MTC4TE/knowledge-graphs.html}
}

@misc{knowledgegraphconstruction2021keynote,
  title = {Keynote: {{Jes\'us Barrasa}} - "{{Knowledge}} Graphs 2021: {{The}} Great Convergence" \#{{KGCW2021}}},
  shorttitle = {Keynote},
  author = {{Knowledge Graph Construction}},
  year = {2021},
  month = sep
}

@article{lopes2022predicting,
  title = {Predicting the Top-Level Ontological Concepts of Domain Entities Using Word Embeddings, Informal Definitions, and Deep Learning{$\blacksquare$}},
  author = {Lopes, Alcides Gon{\c c}alves and Carbonera, Joel Luis and Schimidt, Daniela and Abel, Mara},
  year = {2022},
  month = oct,
  journal = {Expert Systems with Applications: An International Journal},
  volume = {203},
  number = {C},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2022.117291},
  abstract = {Ontology development is a challenging task that encompasses many time-consuming activities. One of these activities is the classification of the domain entities (concepts and instances) according to top-level concepts. This activity is usually performed manually by an ontology engineer. However, when the set of entities increases in size, associating each entity to the proper top-level ontological concept becomes challenging and requires a high level of expertise in both the target domain and ontology engineering. This paper proposes a deep learning approach that automatically classifies domain entities into top-level concepts using their informal definitions and the word embedding of the terms that represent them. From these inputs, we feed a deep neural network consisting of two modules: a feed-forward neural network and a bi-directional recurrent neural network with long short-term units. Our architecture combines both outputs of these modules into a dense layer and provides the probabilities of each candidate class. For validating our proposal, we have developed a dataset based on the OntoWordNet ontology, which provides a classification of WordNet synsets into concepts specified by DOLCE-lite-plus top-level ontology. Our experiments show that our proposal outperforms the baseline approaches by 6\% regarding the F-score. In addition, our proposal is less affected by the polysemy in the terms that represent the domain entities than the compared approaches. Consequently, our proposal can consider more instances during its training than the baseline methods. \textbullet{} Word embedding and deep learning can be used to predict top-level ontology classes. \textbullet{} The polysemy of word vectors can be solved by using informal definitions of the word. \textbullet{} Combining different neural network models have better performance than a single one.},
  keywords = {Deep learning,Ontology learning,Well-founded ontology}
}

@misc{neurips2021,
  title = {{{NeurIPS Data-Centric AI Workshop}}},
  howpublished = {https://datacentricai.org/neurips21/},
  file = {/Users/dkapitan/Zotero/storage/59CE8YHS/neurips21.html}
}

@book{reis2022fundamentals,
  title = {Fundamentals of {{Data Engineering}}},
  abstract = {Data engineering has grown rapidly in the past decade, leaving many software engineers, data scientists, and analysts looking for a comprehensive view of this practice. With this practical book,...},
  isbn = {978-1-09-810829-8},
  langid = {english},
  file = {/Users/dkapitan/Zotero/storage/LFQ43RPC/9781098108298.html}
}

@misc{ruiz201780,
  title = {The 80/20 Data Science Dilemma},
  author = {Ruiz, Armand},
  year = {2017},
  month = sep,
  journal = {InfoWorld},
  abstract = {Most data scientists spend only 20 percent of their time on actual data analysis and 80 percent of their time finding, cleaning, and reorganizing huge amounts of data, which is an inefficient data strategy},
  howpublished = {https://www.infoworld.com/article/3228245/the-80-20-data-science-dilemma.html},
  langid = {english}
}

@book{schreiber1999knowledge,
  title = {Knowledge {{Engineering}} and {{Management}}: {{The CommonKADS Methodology}}},
  shorttitle = {Knowledge {{Engineering}} and {{Management}}},
  author = {Schreiber, Guus and Akkermans, Hans and Anjewierden, Anjo and {de Hoog}, Robert and Shadbolt, Nigel R. and {Van de Velde}, Walter and Wielinga, B. J.},
  year = {1999},
  publisher = {{The MIT Press}},
  doi = {10.7551/mitpress/4073.001.0001},
  isbn = {978-0-262-28323-6},
  langid = {english},
  file = {/Users/dkapitan/Zotero/storage/BVVU8WYM/Schreiber et al. - 1999 - Knowledge Engineering and Management The CommonKA.pdf}
}

@article{sculleyhidden,
  title = {Hidden {{Technical Debt}} in {{Machine Learning Systems}}},
  author = {Sculley, D and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael and Crespo, Jean-Fran{\c c}ois and Dennison, Dan},
  pages = {9},
  abstract = {Machine learning offers a fantastically powerful toolkit for building useful complex prediction systems quickly. This paper argues it is dangerous to think of these quick wins as coming for free. Using the software engineering framework of technical debt, we find it is common to incur massive ongoing maintenance costs in real-world ML systems. We explore several ML-specific risk factors to account for in system design. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, configuration issues, changes in the external world, and a variety of system-level anti-patterns.},
  langid = {english}
}

@misc{sequedaknowledge,
  title = {Knowledge {{Graphs}}},
  author = {Sequeda, Juan F., Claudio Gutierrez},
  abstract = {Tracking the historical events that lead to the interweaving of data and knowledge.},
  howpublished = {https://cacm.acm.org/magazines/2021/3/250711-knowledge-graphs/fulltext},
  langid = {english},
  file = {/Users/dkapitan/Zotero/storage/P4CR6IM8/fulltext.html}
}

@article{stonebrakerdata,
  title = {Data {{Integration}}: {{The Current Status}} and the {{Way Forward}}},
  author = {Stonebraker, Michael and Ilyas, Ihab F},
  pages = {7},
  abstract = {We discuss scalable data integration challenges in the enterprise inspired by our experience at Tamr1. We use multiple real customer examples to highlight the technical difficulties around building a deployable and usable data integration software that tackles the data silos problem. We also highlight the practical aspects involved in using machine learning to enable automating manual or rule-based processes for data integration tasks, such as schema mapping, classification, and deduplication.},
  langid = {english},
  file = {/Users/dkapitan/Zotero/storage/SH54HFDW/Stonebraker and Ilyas - Data Integration The Current Status and the Way F.pdf}
}

@article{studer1998knowledge,
  title = {Knowledge Engineering: {{Principles}} and Methods},
  shorttitle = {Knowledge Engineering},
  author = {Studer, Rudi and Benjamins, V.Richard and Fensel, Dieter},
  year = {1998},
  month = mar,
  journal = {Data \& Knowledge Engineering},
  volume = {25},
  number = {1-2},
  pages = {161--197},
  issn = {0169023X},
  doi = {10.1016/S0169-023X(97)00056-6},
  abstract = {This paper gives an overview of the development of the field of Knowledge Engineering over the last 15 years. We discuss the paradigm shift from a transfer view to a modeling view and describe two approaches which considerably shaped research in Knowledge Engineering: Role-limiting Methods and Generic Tasks. To illustrate various concepts and methods which evolved in recent years we describe three modeling frameworks: CommonKADS, MIKE and PROTI\textasciitilde Gt\textasciitilde -II.This description is supplemented by discussing some important methodological developments in more detail: specification languages for knowledge-based systems, problem-solving methods and ontologies. We conclude by outlining the relationship of Knowledge Engineering to Software Engineering, Information Integration and Knowledge Management.},
  langid = {english},
  file = {/Users/dkapitan/Zotero/storage/6YU8CAIG/Studer et al. - 1998 - Knowledge engineering Principles and methods.pdf}
}

@article{teperek2018data,
  title = {Data {{Stewardship}} \textendash{} Addressing Disciplinary Data Management Needs},
  author = {Teperek, Marta and Cruz, Maria J. and Verbakel, Ellen and B{\"o}hmer, Jasmin K. and Dunning, Alastair},
  year = {2018},
  month = jan,
  publisher = {{OSF}},
  doi = {10.17605/OSF.IO/MJK9T},
  abstract = {One of the biggest challenges for multidisciplinary research institutions which provide data management support to researchers is addressing disciplinary differences1. Centralised services need to be general enough to cater for all the different flavours of research conducted in an institution. At the same time, focusing on the common denominator means that subject-specific differences and needs may not be effectively addressed. In 2017, Delft University of Technology (TU Delft) embarked on an ambitious Data Stewardship project, aiming to comprehensively address data management needs across a multi-disciplinary campus.  In this practice paper, we describe the principles behind the Data Stewardship project at TU Delft, the progress so far, we identify the key challenges and explain our plans for the future.      Hosted on the Open Science Framework},
  langid = {english},
  file = {/Users/dkapitan/Zotero/storage/73IFTIR3/mjk9t.html}
}

@misc{walters2020pathway,
  type = {Text},
  title = {The Pathway towards an {{Information Management Framework}} - {{A}} `{{Commons}}' for {{Digital Built Britain}}},
  author = {Walters, Angela},
  year = {2020},
  month = may,
  abstract = {The Centre for Digital Built Britain's National Digital Twin programme has launched an open consultation seeking feedback on the proposed approach to the development of an Information Management Framework for the built environment.},
  howpublished = {https://www.cdbb.cam.ac.uk/news/pathway-towards-IMF},
  langid = {english},
  file = {/Users/dkapitan/Zotero/storage/P557928J/pathway-towards-IMF.html}
}


